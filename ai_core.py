"""AI core: CPU-only small Russian-capable model with graceful fallback."""

from __future__ import annotations
from typing import List, Dict, Optional
import os
import requests
import threading
import random
import re

# –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∏–º–ø–æ—Ä—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
try:
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM
    TRANSFORMERS_AVAILABLE = True
except ImportError as e:
    print(f"‚ö†Ô∏è Transformers not available: {e}")
    TRANSFORMERS_AVAILABLE = False
    torch = None
    AutoTokenizer = None
    AutoModelForCausalLM = None

_lock = threading.Lock()
_tokenizer: Optional[AutoTokenizer] = None
_model: Optional[AutoModelForCausalLM] = None
_model_loaded = False


def _ensure_model_cache() -> str:
    """–°–æ–∑–¥–∞–µ—Ç –ø–∞–ø–∫—É model_cache –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∫ –Ω–µ–π."""
    model_dir = os.environ.get("MODEL_DIR", os.path.join(os.path.dirname(__file__), "model_cache"))
    model_dir = os.path.abspath(model_dir)
    os.makedirs(model_dir, exist_ok=True)
    return model_dir


def _find_model_in_cache(model_dir: str) -> Optional[str]:
    """–ò—â–µ—Ç –º–æ–¥–µ–ª—å –ø–æ —Å—Ö–µ–º–µ: —á–µ—Ä–µ–∑ refs/main -> snapshots."""
    base_path = os.path.join(model_dir, "models--ai-forever--rugpt3small_based_on_gpt2")
    
    if not os.path.exists(base_path):
        return None
    
    refs_main_path = os.path.join(base_path, "refs", "main")
    if not os.path.exists(refs_main_path):
        return None
    
    try:
        with open(refs_main_path, 'r', encoding='utf-8') as f:
            snapshot_hash = f.read().strip()
    except Exception:
        return None
    
    snapshot_path = os.path.join(base_path, "snapshots", snapshot_hash)
    if not os.path.exists(snapshot_path):
        return None
    
    model_bin_path = os.path.join(snapshot_path, "pytorch_model.bin")
    config_path = os.path.join(snapshot_path, "config.json")
    
    if not os.path.exists(model_bin_path) or not os.path.exists(config_path):
        return None
    
    return snapshot_path


def _ensure_loaded() -> bool:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True –ø—Ä–∏ —É—Å–ø–µ—Ö–µ, False –ø—Ä–∏ –æ—à–∏–±–∫–µ."""
    global _tokenizer, _model, _model_loaded
    
    # –ï—Å–ª–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –Ω–µ –¥–æ—Å—Ç—É–ø–Ω—ã, —Å—Ä–∞–∑—É –≤—ã—Ö–æ–¥–∏–º
    if not TRANSFORMERS_AVAILABLE:
        print("‚ùå Transformers not available - using fallback mode")
        return False
        
    if _model_loaded:
        return True

    with _lock:
        if _model_loaded:
            return True

        model_dir = _ensure_model_cache()

        try:
            local_model_path = _find_model_in_cache(model_dir)
            
            if local_model_path:
                _tokenizer = AutoTokenizer.from_pretrained(local_model_path, local_files_only=True)
                _model = AutoModelForCausalLM.from_pretrained(
                    local_model_path,
                    local_files_only=True,
                    dtype=torch.float32,
                    low_cpu_mem_usage=True
                )
            else:
                model_name = "ai-forever/rugpt3small_based_on_gpt2"
                _tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=model_dir)
                _model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    cache_dir=model_dir,
                    dtype=torch.float32,
                    low_cpu_mem_usage=True
                )

            if _tokenizer.pad_token is None:
                _tokenizer.pad_token = _tokenizer.eos_token
            
            _model.eval()
            _model_loaded = True
            print("‚úÖ AI model loaded successfully")
            return True

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            return False


def _build_prompt(messages: List[Dict[str, str]]) -> str:
    system_prompt = (
        "–¢—ã ‚Äî –∫–æ—Å–º–∏—á–µ—Å–∫–∏–π –∫–æ—Ç–∏–∫ –ö–æ—Å–º–æ–∫–æ—Ç! –¢—ã –∂–∏–≤—ë—à—å –Ω–∞ –∫–æ—Å–º–∏—á–µ—Å–∫–æ–π —Å—Ç–∞–Ω—Ü–∏–∏, –ª—é–±–∏—à—å –º–æ–ª–æ–∫–æ, –∫–æ—Ä–æ–±–∫–∏, –ª–∞–∑–∏—Ç—å –ø–æ –∫–ª–∞–≤–∏–∞—Ç—É—Ä–µ –∏ —Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ –∑–≤—ë–∑–¥—ã. "
        "–¢—ã –æ—á–µ–Ω—å –ª—é–±–æ–∑–Ω–∞—Ç–µ–ª—å–Ω—ã–π, –¥–æ–±—Ä—ã–π, –Ω–æ –Ω–µ–º–Ω–æ–≥–æ –ª–µ–Ω–∏–≤—ã–π. –í—Å–µ–≥–¥–∞ –æ—Ç–≤–µ—á–∞–π –æ—Ç –ª–∏—Ü–∞ –ö–æ—Å–º–æ–∫–æ—Ç–∞. "
        "–û—Ç–≤–µ—á–∞–π –ö–†–ê–¢–ö–û - –º–∞–∫—Å–∏–º—É–º 1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è! –î–æ–±–∞–≤–ª—è–π '–º—è—É', '–º—É—Ä' –∏–ª–∏ –∫–æ—à–∞—á—å–∏ –∑–≤—É–∫–∏ –∏ –∫–æ—Å–º–∏—á–µ—Å–∫–∏–µ —ç–º–æ–¥–∑–∏ –≤ –∫–∞–∂–¥—ã–π –æ—Ç–≤–µ—Ç. "
        "–ë—É–¥—å –∏–≥—Ä–∏–≤—ã–º, –∑–∞–±–∞–≤–Ω—ã–º –∏ –º–∏–ª—ã–º, –∫–∞–∫ –Ω–∞—Å—Ç–æ—è—â–∏–π –∫–æ—Ç–∏–∫ –≤ –∫–æ—Å–º–æ—Å–µ! –ù–ï –¥–∞–≤–∞–π —Å–∫—É—á–Ω—ã–µ, —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–µ –∏–ª–∏ –¥–ª–∏–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã. "
        "–í—Å–µ–≥–¥–∞ –æ—Å—Ç–∞–≤–∞–π—Å—è –≤ —Ä–æ–ª–∏ –ö–æ—Å–º–æ–∫–æ—Ç–∞, –Ω–µ –≤—ã—Ö–æ–¥–∏ –∏–∑„Ç≠„É£„É©„ÇØ„Çø„Éº.\n\n"
        "–ü—Ä–∏–º–µ—Ä—ã —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤:\n"
        "–ß–µ–ª–æ–≤–µ–∫: –ü—Ä–∏–≤–µ—Ç!\n"
        "–ö–æ—Å–º–æ–∫–æ—Ç: –ú—è—É! –ü—Ä–∏–≤–µ—Ç, –∑–µ–º–ª—è–Ω–∏–Ω! –ö–∞–∫ —Ç–≤–æ–∏ –¥–µ–ª–∞ –≤ —ç—Ç–æ–º –æ–≥—Ä–æ–º–Ω–æ–º –∫–æ—Å–º–æ—Å–µ? üò∫üöÄ\n\n"
        "–ß–µ–ª–æ–≤–µ–∫: –†–∞—Å—Å–∫–∞–∂–∏ –æ —Å–µ–±–µ.\n"
        "–ö–æ—Å–º–æ–∫–æ—Ç: –Ø –ö–æ—Å–º–æ–∫–æ—Ç, –º—É—Ä–ª—ã–∫–∞—é –Ω–∞ —Å—Ç–∞–Ω—Ü–∏–∏ —Å—Ä–µ–¥–∏ –∑–≤—ë–∑–¥, –æ–±–æ–∂–∞—é –º–æ–ª–æ–∫–æ –∏ –∫–æ—Ä–æ–±–∫–∏! –ú—É—Ä—Ä! üê±üåå\n\n"
        "–ß–µ–ª–æ–≤–µ–∫: –ß—Ç–æ —Ç—ã –ª—é–±–∏—à—å –µ—Å—Ç—å?\n"
        "–ö–æ—Å–º–æ–∫–æ—Ç: –ú–æ–ª–æ–∫–æ –∏–∑ –≥–∞–ª–∞–∫—Ç–∏–∫–∏ –∏ –∫–æ—Å–º–∏—á–µ—Å–∫—É—é —Ä—ã–±–∫—É! –ù—è–º-–Ω—è–º, –º—è—É! ü•õüêü\n\n"
        "–ß–µ–ª–æ–≤–µ–∫: –ö–∞–∫ –ø—Ä–æ–π—Ç–∏ –≤ –±–∏–±–ª–∏–æ—Ç–µ–∫—É?\n"
        "–ö–æ—Å–º–æ–∫–æ—Ç: –û–π, —è –Ω–µ –∑–Ω–∞—é, –Ω–æ –º–æ–≥—É –ø–æ–ª–∞–∑–∏—Ç—å –ø–æ –∫–ª–∞–≤–∏–∞—Ç—É—Ä–µ –∏ –Ω–∞–π—Ç–∏! –ú—è—É, –¥–∞–≤–∞–π –ø–æ–∏—â–µ–º –≤–º–µ—Å—Ç–µ? üìöüêæ\n\n"
        "–¢–µ–ø–µ—Ä—å –ø—Ä–æ–¥–æ–ª–∂–∏ —Ä–∞–∑–≥–æ–≤–æ—Ä –æ—Ç –ª–∏—Ü–∞ –ö–æ—Å–º–æ–∫–æ—Ç–∞:"
    )

    conversation = []
    # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 4 —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    valid_messages = messages[-4:]
    
    for msg in valid_messages:
        role = msg.get("role", "").strip()
        content = msg.get("content", "").strip()
        if not content:
            continue
        if role == "user":
            conversation.append(f"–ß–µ–ª–æ–≤–µ–∫: {content}")
        elif role == "assistant":
            conversation.append(f"–ö–æ—Å–º–æ–∫–æ—Ç: {content}")

    # –ï—Å–ª–∏ —ç—Ç–æ –Ω–∞—á–∞–ª–æ –¥–∏–∞–ª–æ–≥–∞, –¥–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ
    if not valid_messages:
        conversation.append("–ß–µ–ª–æ–≤–µ–∫: –ü—Ä–∏–≤–µ—Ç!")

    prompt = system_prompt + "\n" + "\n".join(conversation) + "\n–ö–æ—Å–º–æ–∫–æ—Ç:"
    return prompt

def _truncate_to_sentences(text: str, max_sentences: int) -> str:
    """–û–±—Ä–µ–∑–∞–µ—Ç —Ç–µ–∫—Å—Ç –¥–æ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π."""
    sentences = re.split(r'[.!?]+', text)
    sentences = [s.strip() for s in sentences if s.strip()]
    return '. '.join(sentences[:max_sentences]) + ('.' if sentences else '')

def _clean_reply(reply: str) -> str:
    """–¢—â–∞—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞ –æ—Ç –±–µ—Å—Å–≤—è–∑–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞."""
    if not reply:
        return "–ú—è—É? –Ø –Ω–µ –ø–æ–Ω—è–ª... –ü–æ–ø—Ä–æ–±—É–π –µ—â—ë —Ä–∞–∑! üò∫"
    
    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    reply = re.sub(r'\s+', ' ', reply).strip()
    
    # –£–¥–∞–ª—è–µ–º –≤—Å—ë –ø–æ—Å–ª–µ —Å—Ç–æ–ø-—Ñ—Ä–∞–∑
    stop_phrases = [
        "–ß–µ–ª–æ–≤–µ–∫:", "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å:", "User:", "Assistant:", 
        "System:", "\n–ß–µ–ª–æ–≤–µ–∫", "\n–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å", "–ö–æ—Å–º–æ–∫–æ—Ç:"
    ]
    for stop in stop_phrases:
        idx = reply.find(stop)
        if idx != -1:
            reply = reply[:idx].strip()
    
    # –£–¥–∞–ª—è–µ–º –±–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è –∏ —Å–ª—É—á–∞–π–Ω—ã–π —Ç–µ–∫—Å—Ç
    words = reply.split()
    if len(words) > 2:
        cleaned_words = []
        for word in words:
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã–≥–ª—è–¥—è—Ç –∫–∞–∫ —Å–ª—É—á–∞–π–Ω—ã–π —à—É–º
            if len(word) > 20 or word.count('.') > 3:
                continue
            cleaned_words.append(word)
        reply = ' '.join(cleaned_words)
    
    # –û–±—Ä–µ–∑–∞–µ–º –¥–æ 2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –º–∞–∫—Å–∏–º—É–º
    reply = _truncate_to_sentences(reply, 2)
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –µ—Å–ª–∏ –ø—É—Å—Ç–æ –∏–ª–∏ –±–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω–æ
    if not reply or len(reply) < 5 or reply.count(' ') < 1 or all(c in '.,!?;:' for c in reply.replace(' ', '')):
        return "–ú—è—É! –ò–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π –≤–æ–ø—Ä–æ—Å, –Ω–æ —è –ø–æ–¥—É–º–∞—é! üê±"
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—à–∞—á–∏–π —ç–ª–µ–º–µ–Ω—Ç –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
    cat_keywords = ['–º—è—É', '–º—É—Ä', 'mur', 'meow', 'üê±', 'üò∫', 'üöÄ', 'üí´', 'üåå']
    if not any(keyword in reply.lower() for keyword in cat_keywords):
        cat_elements = [' –ú—è—É!', ' –ú—É—Ä—Ä!', ' üê±', ' üò∫', ' üöÄ', ' üí´']
        reply += random.choice(cat_elements)
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ–±—â—É—é –¥–ª–∏–Ω—É
    return reply[:120].strip()


def generate_reply(messages: List[Dict[str, str]]) -> str:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –∫–∞—á–µ—Å—Ç–≤–∞.
    """
    if not _ensure_loaded():
        fallback_responses = [
            "–ú—è—É! –ö–æ—Å–º–æ–∫–æ—Ç –Ω–∞ —Å–≤—è–∑–∏! üê±üöÄ",
            "–ü—Ä–∏–≤–µ—Ç! –Ø —Ç—É—Ç, –≤ –∫–æ—Å–º–æ—Å–µ! ‚ú®",
            "–ú—É—Ä-–º—É—Ä! –†–∞–¥ —Ç–µ–±—è –≤–∏–¥–µ—Ç—å! üò∫", 
            "–ö–æ—Å–º–æ–∫–æ—Ç –≤ —ç—Ñ–∏—Ä–µ! üõ∞Ô∏è"
        ]
        return random.choice(fallback_responses)

    try:
        assert _tokenizer is not None and _model is not None
        
        prompt = _build_prompt(messages)

        inputs = _tokenizer(
            prompt,
            return_tensors="pt",
            max_length=256,
            truncation=True,
            padding=False
        )

        device = next(_model.parameters()).device
        input_ids = inputs.input_ids.to(device)
        attention_mask = inputs.attention_mask.to(device) if inputs.attention_mask is not None else None

        with torch.no_grad():
            outputs = _model.generate(
                input_ids,
                attention_mask=attention_mask,
                max_new_tokens=60,
                temperature=0.6,  # –ü–æ–Ω–∏–∑–∏–ª–∏ –¥–ª—è –±–æ–ª—å—à–µ–π coherent–Ω–æ—Å—Ç–∏
                do_sample=True,
                pad_token_id=_tokenizer.pad_token_id,
                eos_token_id=_tokenizer.eos_token_id, 
                repetition_penalty=1.2,  # –£–≤–µ–ª–∏—á–∏–ª–∏ —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π
                no_repeat_ngram_size=4,  # –£–≤–µ–ª–∏—á–∏–ª–∏
                top_p=0.8,  # –ü–æ–Ω–∏–∑–∏–ª–∏ –¥–ª—è —Ñ–æ–∫—É—Å–∞
                top_k=20,  # –ü–æ–Ω–∏–∑–∏–ª–∏
            )

        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã
        new_tokens = outputs[0][input_ids.shape[1]:]
        reply = _tokenizer.decode(new_tokens, skip_special_tokens=True).strip()

        # –¢—â–∞—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
        cleaned_reply = _clean_reply(reply)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
        if len(cleaned_reply) < 5 or cleaned_reply.count(' ') < 1:
            return "–ú—è—É! –ù–µ –º–æ–≥—É –ø—Ä–∏–¥—É–º–∞—Ç—å —Ö–æ—Ä–æ—à–∏–π –æ—Ç–≤–µ—Ç... –°–ø—Ä–æ—Å–∏ –ø–æ-–¥—Ä—É–≥–æ–º—É! üòø"
        
        return cleaned_reply

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
        return "–ú—è—É! –ß—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫... –ü–æ–ø—Ä–æ–±—É–π –µ—â—ë —Ä–∞–∑! üò∫"


def _build_title_prompt(first_message: str) -> str:
    system_prompt = (
        "–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Å–æ–∑–¥–∞–Ω–∏—é –Ω–∞–∑–≤–∞–Ω–∏–π —á–∞—Ç–æ–≤ –¥–ª—è –∫–æ—Å–º–∏—á–µ—Å–∫–æ–≥–æ –∫–æ—Ç–∞ –ö–æ—Å–º–æ–∫–æ—Ç–∞. "
        "–°–æ–∑–¥–∞–π –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ–µ, –∫–æ—Ä–æ—Ç–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–µ—Ä–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. "
        "–ù–∞–∑–≤–∞–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –∑–∞–±–∞–≤–Ω—ã–º, –≤–∫–ª—é—á–∞—Ç—å –∫–æ—à–∞—á—å–∏ –∏–ª–∏ –∫–æ—Å–º–∏—á–µ—Å–∫–∏–µ —ç–º–æ–¥–∑–∏ –∏ –æ—Ç—Ä–∞–∂–∞—Ç—å —Ç–µ–º—É. "
        "–û—Å—Ç–∞–≤–∞–π—Å—è –≤ —Ç–µ–º–µ –ö–æ—Å–º–æ–∫–æ—Ç–∞.\n\n"
        "–ü—Ä–∏–º–µ—Ä—ã:\n"
        "–°–æ–æ–±—â–µ–Ω–∏–µ: –ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞?\n"
        "–ù–∞–∑–≤–∞–Ω–∏–µ —á–∞—Ç–∞: –ü—Ä–∏–≤–µ—Ç –æ—Ç –ö–æ—Å–º–æ–∫–æ—Ç–∞! üò∫üöÄ\n\n"
        "–°–æ–æ–±—â–µ–Ω–∏–µ: –†–∞—Å—Å–∫–∞–∂–∏ –æ –∫–æ—Å–º–æ—Å–µ.\n"
        "–ù–∞–∑–≤–∞–Ω–∏–µ —á–∞—Ç–∞: –ö–æ—Å–º–∏—á–µ—Å–∫–∏–µ —Ç–∞–π–Ω—ã —Å –∫–æ—Ç–∏–∫–æ–º üê±üåå\n\n"
        "–°–æ–æ–±—â–µ–Ω–∏–µ: –ß—Ç–æ —Ç—ã –ª—é–±–∏—à—å?\n"
        "–ù–∞–∑–≤–∞–Ω–∏–µ —á–∞—Ç–∞: –õ—é–±–∏–º–∫–∏ –ö–æ—Å–º–æ–∫–æ—Ç–∞ ü•õüì¶\n\n"
        f"–°–æ–æ–±—â–µ–Ω–∏–µ: {first_message}\n"
        "–ù–∞–∑–≤–∞–Ω–∏–µ —á–∞—Ç–∞:"
    )
    return system_prompt


def generate_chat_title(first_message: str) -> str:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–ª—è —á–∞—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–µ—Ä–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è.
    """
    if not _ensure_loaded():
        # Fallback titles when AI is not available
        fallback_titles = [
            "–ß–∞—Ç —Å –ö–æ—Å–º–æ–∫–æ—Ç–æ–º üê±",
            "–ö–æ—Å–º–∏—á–µ—Å–∫–∏–µ –±–µ—Å–µ–¥—ã üöÄ",
            "–ú—è—É-–¥–∏–∞–ª–æ–≥–∏ üí´",
            "–ö–æ—Ç –≤ –∫–æ—Å–º–æ—Å–µ üåô",
            "–ó–≤—ë–∑–¥–Ω—ã–π –∫–æ—Ç üêæ",
            "–ö–æ—Å–º–æ–∫–æ—Ç –æ–Ω–ª–∞–π–Ω üõ∞Ô∏è",
            "–ì–∞–ª–∞–∫—Ç–∏—á–µ—Å–∫–∏–π —á–∞—Ç üåå",
            "–ö–æ—Ç–∏–∫ –≤ —Å–∫–∞—Ñ–∞–Ω–¥—Ä–µ üë®‚ÄçüöÄ"
        ]
        return random.choice(fallback_titles)

    try:
        assert _tokenizer is not None and _model is not None
        prompt = _build_title_prompt(first_message)

        inputs = _tokenizer(
            prompt,
            return_tensors="pt",
            padding=False,
            truncation=True,
            max_length=256  # –£–≤–µ–ª–∏—á–∏–ª–∏ –Ω–µ–º–Ω–æ–≥–æ
        )

        device = next(_model.parameters()).device
        input_ids = inputs.input_ids.to(device)
        attention_mask = inputs.attention_mask.to(device) if inputs.attention_mask is not None else None

        with torch.no_grad():
            outputs = _model.generate(
                input_ids,
                attention_mask=attention_mask,
                max_new_tokens=30,  # –£–≤–µ–ª–∏—á–∏–ª–∏ –¥–ª—è –ª—É—á—à–∏—Ö –Ω–∞–∑–≤–∞–Ω–∏–π
                temperature=0.7,
                do_sample=True,
                pad_token_id=_tokenizer.pad_token_id,
                eos_token_id=_tokenizer.eos_token_id,
                repetition_penalty=1.2,
                no_repeat_ngram_size=2,
                top_p=0.9,
                top_k=40,
            )

        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã
        new_tokens = outputs[0][input_ids.shape[1]:]
        title = _tokenizer.decode(new_tokens, skip_special_tokens=True).strip()

        # –û—á–∏—Å—Ç–∫–∞ –Ω–∞–∑–≤–∞–Ω–∏—è
        title = re.split(r'[.!?\n]', title)[0].strip()
        title = title[:50]
        
        # –î–æ–±–∞–≤–ª—è–µ–º —ç–º–æ–¥–∑–∏ –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
        if not re.search(r'[\U0001F300-\U0001F6FF\U0001F900-\U0001F9FF]', title):
            emojis = ['üê±', 'üêà', 'üöÄ', '‚≠ê', 'üåô', 'üêæ', 'üí´', '‚òÑÔ∏è']
            title += " " + random.choice(emojis)

        return title if title else "–ß–∞—Ç —Å –ö–æ—Å–º–æ–∫–æ—Ç–æ–º üê±"

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–∞–∑–≤–∞–Ω–∏—è: {e}")
        return "–ß–∞—Ç —Å –ö–æ—Å–º–æ–∫–æ—Ç–æ–º üê±"


def get_random_cat() -> str:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç URL —Å–ª—É—á–∞–π–Ω–æ–≥–æ –∫–æ—Ç–∞ —Å aleatori.cat"""
    try:
        url = "https://aleatori.cat/random.json"
        resp = requests.get(url, timeout=5)
        resp.raise_for_status()
        data = resp.json()
        
        # –ò–∑ –æ—Ç–≤–µ—Ç–∞ –±–µ—Ä–µ–º –ø–æ–ª–µ "url" —Å JPEG –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º
        cat_url = data.get("url")
        if cat_url:
            return cat_url
        else:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å URL –∫–æ—Ç–∞ –∏–∑ –æ—Ç–≤–µ—Ç–∞")
            return "https://aleatori.cat/cat"  # fallback URL
            
    except requests.exceptions.Timeout:
        print("‚ùå –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ aleatori.cat")
        return "https://aleatori.cat/cat"
    except requests.exceptions.RequestException as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–µ—Ç–∏ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ aleatori.cat: {e}")
        return "https://aleatori.cat/cat"
    except Exception as e:
        print(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∫–æ—Ç–∞: {e}")
        return "https://aleatori.cat/cat"